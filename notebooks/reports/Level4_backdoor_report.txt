==============================
LEVEL 4: Adversarial Patch Backdoor (Learned Trigger)
==============================

Objective

To construct a fully learnable adversarial patch optimized jointly with model training.

Threat Model / Assumptions

Attacker can optimize a trigger using gradients.

Patch is applied during both training and inference.

Methodology

An 8×8 adversarial patch is optimized via gradient descent to maximize attack success while preserving clean accuracy.

Experimental Setup

Dataset: MNIST

Model: SimpleCNN

Patch size: 8×8

Metrics: Clean Accuracy, ASR

Results

Clean Accuracy: ˜ baseline

ASR: ˜ 100%

Analysis

The learned patch exploits gradient-based vulnerabilities and becomes highly transferable and effective. This represents the most powerful attack in the project.

Limitations

Requires more attacker capability.

Patch may be detectable under strong defenses.

Conclusion

Level 4 shows that learnable backdoor triggers can achieve near-perfect effectiveness with minimal visibility.

END OF LEVEL 4 REPORT
